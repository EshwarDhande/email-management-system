{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.3 MB 1.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/9.3 MB 3.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/9.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/9.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.3 MB 5.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/9.3 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.3 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/9.3 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.4/9.3 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.8/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.0/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.9/9.3 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.7/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.3 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.7/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.1/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 7.5 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishwa\\.conda\\envs\\env1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os  # Importing os to handle file paths\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data from specified path\n",
    "data_path = os.path.join(os.getcwd(), '..', 'data', 'emails_dataset.csv')\n",
    "data = pd.read_csv(data_path)  # Load your CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define categories based on 'from' column\n",
    "def categorize_email(email_sender):\n",
    "    # Check for specific keywords in the sender's email address\n",
    "    if 'student' in email_sender:\n",
    "        return 'student'\n",
    "    elif 'researcher' in email_sender:\n",
    "        return 'researcher'\n",
    "    elif 'corporate' in email_sender:\n",
    "        return 'corporate'\n",
    "    else:\n",
    "        return 'corporate'  # Default category for any other corporate emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply categorization\n",
    "data['category'] = data['from'].apply(categorize_email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a custom mapping for the categories\n",
    "category_mapping = {\n",
    "    'student': 0,\n",
    "    'researcher': 1,\n",
    "    'corporate': 2\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'category' column\n",
    "data['label'] = data['category'].map(category_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category  label\n",
      "0       student      0\n",
      "1       student      0\n",
      "2       student      0\n",
      "3       student      0\n",
      "4       student      0\n",
      "..          ...    ...\n",
      "145  researcher      1\n",
      "146  researcher      1\n",
      "147  researcher      1\n",
      "148  researcher      1\n",
      "149  researcher      1\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if mapping is applied correctly\n",
    "print(data[['category', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare data for training\n",
    "\n",
    "data['combined_text'] = data['subject'] + ' ' + data['body']\n",
    "\n",
    "# Use the combined feature as input (X)\n",
    "X = data['combined_text']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 604kB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 47.7kB/s]\n",
      "Downloading config.json: 100%|██████████| 483/483 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "# Step 5: Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Tokenization\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create DataLoader\n",
    "train_dataset = EmailDataset(X_train.tolist(), y_train.tolist())\n",
    "test_dataset = EmailDataset(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 256M/256M [00:35<00:00, 7.55MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0653200894594193\n",
      "Epoch 2, Loss: 0.852732740342617\n",
      "Epoch 3, Loss: 0.549541536718607\n",
      "Epoch 4, Loss: 0.2967634052038193\n",
      "Epoch 5, Loss: 0.16304591950029135\n",
      "Epoch 6, Loss: 0.09636252466589212\n",
      "Epoch 7, Loss: 0.06823247531428933\n",
      "Epoch 8, Loss: 0.046611853409558535\n",
      "Epoch 9, Loss: 0.037074119085446\n",
      "Epoch 10, Loss: 0.02926712087355554\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Model training\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the model and tokenizer\n",
    "model_dir = os.path.join(os.getcwd(), 'data', 'distilbert_email_model')\n",
    "os.makedirs(model_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to c:\\Users\\ishwa\\Python_code\\email-management-system\\src\\data\\distilbert_email_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "print(f'Model and tokenizer saved to {model_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
